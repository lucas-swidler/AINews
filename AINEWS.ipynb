{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AINEWS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports:\n",
        "Necessary to run this cell for the rest of the notebook"
      ],
      "metadata": {
        "id": "_iSIEocVLdOr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I10YXsqkJzWq",
        "outputId": "6255bc44-dcf6-43d3-e95e-e92fedd574ac"
      },
      "source": [
        "# Import relevant libraries\n",
        "# Article scraping\n",
        "!pip install newspaper3k\n",
        "import newspaper\n",
        "\n",
        "# Deep learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# Various useful libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 27.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Collecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.4.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13552 sha256=a88ba28e78192bc6f2b188cc2971b8a4a7c691c7a453e17d378fc9210f35865f\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3356 sha256=d4f3bf07f6d185a5be63231eea3bee376a04306ff712c4ee54486d23a4267d14\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398405 sha256=953792a2087b708846eb4fe04c9dba5ad35db479d2ff0a44a51941d304eadd30\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=0d98992e7fc7ad30233701544267b9e96d56bf627cf4655cb6da7a85740f283f\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: sgmllib3k, requests-file, tldextract, tinysegmenter, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.8 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.2\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the Dataset:\n",
        "This section of code **only needs to be run once**\n",
        "\n",
        "It creates text documents containing the text of articles in one, and the titles of articles in the other"
      ],
      "metadata": {
        "id": "iA_dNrRHLhoY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU-3xl0LKLN0"
      },
      "source": [
        "# This does not need to be run anymore, as we already have a text document with data in it!\n",
        "# Next, in order to save time scraping articles all day, we'll make a text file with the relevant information from the articles\n",
        "# Create a news pool\n",
        "from newspaper import news_pool\n",
        "\n",
        "# Lists of information we'll use later\n",
        "articles_text = []\n",
        "articles_authors = []\n",
        "articles_titles = []\n",
        "\n",
        "# List of sources: can be extended or shortened as needed\n",
        "# memorize_articles shouldn't matter but I still included it\n",
        "# I could add keep_article_html=True as a parameter to keep the html, meaning I could have AI generated html... i'll test this once I have articles being made\n",
        "cbs_paper = newspaper.build('http://cbs.com', memoize_articles = False)\n",
        "slate_paper = newspaper.build('http://slate.com', memoize_articles = False)\n",
        "espn_paper = newspaper.build('http://espn.com', memoize_articles = False)\n",
        "\n",
        "papers = [cbs_paper, slate_paper, espn_paper]\n",
        "news_pool.set(papers, threads_per_source = 2)\n",
        "news_pool.join() # Note: this line takes roughly 3 and a half minutes to run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbBUcuedLtaL"
      },
      "source": [
        "# This does not need to be run anymore, as we already have a text document with data in it!\n",
        "# Now that we have the articles, let's make 2 text documents with information we'll need: Article title and article text\n",
        "for paper in papers: # Note: this for loop takes roughly a minute and a half to run\n",
        "  for item in range(paper.size()):\n",
        "    articles = paper.articles[item]\n",
        "    articles.parse()\n",
        "    articles_text.append(articles.text)\n",
        "    articles_titles.append(articles.title)\n",
        "    articles_authors.append(articles.authors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QcWObORlhRV"
      },
      "source": [
        "# This does not need to be run anymore, as we already have a text document with data in it!\n",
        "print(articles_titles, file=open('articletitles.txt', 'w'))\n",
        "print(articles_text, file=open('articletext.txt', 'w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the dataset created above\n",
        "path_to_file = '/content/drive/MyDrive/AINewsData/articletext.txt' #change to fit where you put your file\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Print out the length of the data, along with the unique characters\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gkxvywdMP1k",
        "outputId": "4da9ed41-6f84-4f97-fd38-dcaeb2f3f0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 4777890 characters\n",
            "240 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the Model:\n",
        "Source: https://www.tensorflow.org/text/tutorials/text_generation"
      ],
      "metadata": {
        "id": "OaI5waPWL-ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vectorization:"
      ],
      "metadata": {
        "id": "mC4tDNEg1iix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we want to vectorize the data, in other words we'll be assigning a number value for each letter\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om1Sn9HL0_6O",
        "outputId": "ea339e87-8f71-4b57-c2e3-591c73a0c4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "ids = ids_from_chars(chars)\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "NQifG2tzOd3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Examples:"
      ],
      "metadata": {
        "id": "IeaKUB_C1oZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to create examples of text that the RNN can train on\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08BVNrJ91p8r",
        "outputId": "61d1daf5-000e-4d21-d11f-87fd4a68c93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "'\n",
            "'\n",
            ",\n",
            " \n",
            "'\n",
            "S\n",
            "t\n",
            "e\n",
            "p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using longer sequences\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbrpb5YI2J9i",
        "outputId": "6e809dcb-797b-4098-c442-67f262a85e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"['', 'Stephen Colbert brings his signature satire and comedy to The Late Show with Stephen Colbert, t\"\n",
            "b'he #1 show in late night, where he talks with an eclectic mix of guests about what is new and relevan'\n",
            "b't in the worlds of politics, entertainment, business, music, technology, and more. Featuring bandlead'\n",
            "b'er Jon Batiste with his band Stay Human, the Emmy Award-nominated show is broadcast from the historic'\n",
            "b' Ed Sullivan Theater. Stephen Colbert, Chris Licht, Tom Purcell, and Jon Stewart are executive produc'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "For training you'll need a dataset of (input, label) pairs where input and label are sequences. \n",
        "At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:\n",
        "'''\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n"
      ],
      "metadata": {
        "id": "LfoC6GjC2YzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDeHvi5W2vrH",
        "outputId": "71c274f7-eb51-4e0e-85ad-c072b403e4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b\"['', 'Stephen Colbert brings his signature satire and comedy to The Late Show with Stephen Colbert, \"\n",
            "Target: b\"'', 'Stephen Colbert brings his signature satire and comedy to The Late Show with Stephen Colbert, t\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Training Batches:"
      ],
      "metadata": {
        "id": "Up67cN_72ywW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIknWkEL3dbw",
        "outputId": "983f3f47-ea99-44de-e534-2bcc3382a0e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building the RNN Model:"
      ],
      "metadata": {
        "id": "DZ7nwv7l3iAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As we have a large dataset and we want to maintain some level of memory, we'll use a GRU RNN here\n",
        "# Additionally, as the internal state will need to be maintained, we will be defining the RNN manually rather than just using a sequential model\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "ow_JMvCJ3j3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model class using TF and Keras\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "ymewEqZc5eRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "jlxl0mPq5jke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXaTMttd5qzw",
        "outputId": "f477e132-6336-4120-ca38-ab92ed93f295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 241) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  61696     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  247025    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,247,025\n",
            "Trainable params: 4,247,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the Model:"
      ],
      "metadata": {
        "id": "HG2MBehQ53XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have the model, we need to actually train it so it can make any actual predictions\n",
        "# Loss Function\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRQAu2mM55Mn",
        "outputId": "a24ac9fa-894f-4a4d-ad19-8df5bd75c5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 241)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         5.485957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile with Adam optimizer, otherwise default\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "yRrz8YrE6dy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we'll be setting up checkpoints so that we can return to a previous state of the model if something goes wrong\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = '/content/drive/MyDrive/AINewsData'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n"
      ],
      "metadata": {
        "id": "wDmTkWHF6n2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training - 20 epochs for higher accuracy\n",
        "history = model.fit(dataset, epochs=20, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js_zKj2d6159",
        "outputId": "f6097e2a-b81e-4936-eff8-0bdaf410a08a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "739/739 [==============================] - 111s 140ms/step - loss: 2.1459\n",
            "Epoch 2/20\n",
            "739/739 [==============================] - 104s 139ms/step - loss: 1.4724\n",
            "Epoch 3/20\n",
            "739/739 [==============================] - 104s 138ms/step - loss: 1.3155\n",
            "Epoch 4/20\n",
            "739/739 [==============================] - 104s 139ms/step - loss: 1.2340\n",
            "Epoch 5/20\n",
            "739/739 [==============================] - 104s 138ms/step - loss: 1.1764\n",
            "Epoch 6/20\n",
            "739/739 [==============================] - 104s 139ms/step - loss: 1.1313\n",
            "Epoch 7/20\n",
            "739/739 [==============================] - 104s 139ms/step - loss: 1.0940\n",
            "Epoch 8/20\n",
            "739/739 [==============================] - 104s 139ms/step - loss: 1.0634\n",
            "Epoch 9/20\n",
            "739/739 [==============================] - 104s 138ms/step - loss: 1.0386\n",
            "Epoch 10/20\n",
            "739/739 [==============================] - 104s 138ms/step - loss: 1.0185\n",
            "Epoch 11/20\n",
            "739/739 [==============================] - 104s 139ms/step - loss: 1.0034\n",
            "Epoch 12/20\n",
            "739/739 [==============================] - 104s 139ms/step - loss: 0.9918\n",
            "Epoch 13/20\n",
            "739/739 [==============================] - 104s 139ms/step - loss: 0.9842\n",
            "Epoch 14/20\n",
            "739/739 [==============================] - 104s 138ms/step - loss: 0.9786\n",
            "Epoch 15/20\n",
            "739/739 [==============================] - 104s 138ms/step - loss: 0.9770\n",
            "Epoch 16/20\n",
            "739/739 [==============================] - 104s 138ms/step - loss: 0.9786\n",
            "Epoch 17/20\n",
            "739/739 [==============================] - 104s 139ms/step - loss: 0.9815\n",
            "Epoch 18/20\n",
            "739/739 [==============================] - 104s 138ms/step - loss: 0.9879\n",
            "Epoch 19/20\n",
            "739/739 [==============================] - 103s 138ms/step - loss: 0.9988\n",
            "Epoch 20/20\n",
            "739/739 [==============================] - 104s 138ms/step - loss: 1.0075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Predicted Text:"
      ],
      "metadata": {
        "id": "DGzBjAS8B5av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "JOCSEgj5B7S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "uNoPDRKlB-pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Body:', 'Body:', 'Body:', 'Body:', 'Body:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rSnoNQXCBg5",
        "outputId": "c66c92e9-99a9-4d14-a226-770ea40d10c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Body: Belling all day he has denyer to bowl from Argentine champions I was take nor here that lifted him during that rap window paymead.\\\\n\\\\n\"It took a message as well as he explains that the franchise to leave, sometimes of Drey Jones, GT. Wose named Pep Guardiola began, there are also me well was as \\xe2\\x80\\x9cmuch not a genuine injury in some of the field.\\\\n\\\\nI\\\\\\'d seen a nodd from the top biggest highest promotion said.\\\\n\\\\nBradford: 15-9\\\\n\\\\nGamp of that mistaka hints at the WSL (2014 and Curry (c), or Mexico) in them have a great older right customal, too kicked a foot common consistent ingraside part of the top best when an injury against Umark Choights -- UFC strawe from the 2019 Cavaliers were on the program and insists to the 2016-20 season. Aldres settled in the last winner. \"If there are not predictable after him her goal on our stories. The free that I just take better everything happen before here being talked and then doing this, it was an vehable to look at how long ethere by still winnin'\n",
            " b'Body: She tests promisely I criteral manager who returned from seen, I thought that always said. \"It is usually performed when I become it\\xe2\\x80\\x99s going to be general manntage but not help [pressed] after a happy more,\" Ferentz said. \"And so they say it.\\\\n\\\\nIn uniform, figure to country cauched the employe offensive player.\\\\n\\\\nThe FIA facilized positions by more than just four episodes of their parkings: Durann was off to his book room for the most dufficity in many honors.\\\\n\\\\nOther notable stars: Premier Leagu\\\\\\'s artists and ESPN2 was also hoping to get a photo with unaided to get Wripty. Kuema finished Returned to Barcelona and then a discussion. There\\\\\\'s doing what has had offer to graterol pro wrestling and significant upset of six expensed to continue. He wasn\\\\\\'t routed on his own team champion after a hard-one, the No. 1 2,000 conversation in late 2019 and left for tests drama to Spain history. After Masi decoral range of coaches, Chisoro chasing the right gentematic. Instead of Spellouse i'\n",
            " b\"Body: Carlos Police, Chiefs -2 (Giannis Antetop & Second Court) years ago, Multipace Rependous (1997-2003)\\\\n\\\\nCareer seasons en cusi coming debulate defensive gray set sharts weight class.\\\\n\\\\n- What\\\\'s been very long before I showed us to him is ottended; pushing a catapult its University and family.\\\\n\\\\nBOMISS WELL, in November needs the team\\\\'s coach knew that the time as Team Raw (Shewcainer\\xe2\\x80\\x9d McWith birst round. Eimothing was basically big to address each of himself in 2011 and is remained (and worldwide) on the ground for not for WWE.\\\\n\\\\nSure Tribes, who has not quite nicely spreading with PCL, Seth. Gallah, or have to create off the court, the NBA\\\\'s Venio Steach\\\\n\\\\n2010 results\\\\n\\\\nGraduation, at vehisling, Ewen rolled for data at the chambio, being the team learned: \\xe2\\x80\\x9cThe Year Grandma it was a plation in UFC \\xe2\\x80\\x94 MMA (36-6, 2 pounds)\\\\n\\\\nPromotion: UFC champion\\\\nRecord: 21-8\\\\n\\\\nLast: L (UD12) Socter (invarcators, WWE, Page, Olumman, Denz natria) writter hit 39 championships. How (a third co\"\n",
            " b\"Body: On the campetition even (as middle road was talking about the frinting) of the manager. The rematch is important to the Miami, it is she imaginable afterward. ESPN\\\\n\\\\nSecond flyshing jag pick bag with his nine minor loss or view or a front kind of backup 50s winning the UFC in 2021 -- to come out of November this season, man -- a binarous counter-moved from based on his team\\\\n\\\\nIt can cost $310,000? Then he last since the race director Michael Washington. He contentioned at the end of her nice atressed on to watch for that. Joe Louis Jr. proceed infortance in all the being spotters in 2022, after the Ravens and Chaig Outter JacMice to search the first time. The GoldengenPro/Bump Burtham is a message to her generation. Because of the beginning to the team leave, Dan, he see it is no longer charm, I\\\\'d stay, which to a first-round club from 8 points, and only three years in the hospital lat, but there were a whistle -- and that included herself now and after quite links, as he did just \"\n",
            " b\"Body: LaCava (AJC), Gayon Ronaldo, Nick Cuernes (@Siachbook!) Masvina, Running Final, MFG Tourna, Dec. 18, 2020\\\\n\\\\nNext: TBD\\\\n\\\\n4. Jericho Oliveira has graded her seven-figured game around Double Douglas\\\\' stock bets in the safety car on Thursday, and the game. A few minutes have confirmed her the objects out of the final on his career.\\\\n\\\\nIntrontual manager Rob Homa, Sandy Bradley Jr. is the club lightning that it doesn\\\\'t the new pathold just behind with his size.\\\\n\\\\nBrock Lewellin and change previews gave the players see to get it down. Toom this fight not. Mighthing hopes after which the agreement began to stop satired as Iowa will always be leaving? The nexes at the top three in April going Super Soused\\xe2\\x80\\x99s most promo bounces, as one of the most knowledge that a 76-man Bowl. Men's fight between two of the right shoes, leading the far too much coring to the pinnacle \\xe2\\x80\\x94 carry over Liga MX, but armed tweather Aaron Rodgers/Getty Images\\\\n\\\\nThe WNBA team has been friends in a title.\\\\n\\\\nThe Ang\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.474995136260986\n"
          ]
        }
      ]
    }
  ]
}